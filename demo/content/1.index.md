---
navigation.title: '笔记'
---

# 短学期学习笔记

## 一、流行框架

> [深度学习三大框架之争](https://zhuanlan.zhihu.com/p/364670970)

### 1.1 简介

1. `TensorFlow`：由`Google`开发的一个用于数值计算和深度学习的开源软件库，支持多种编程语言和平台，可以构建、训练和部署各种机器学习模型。
2. `PyTorch`：由`Facebook`支持的一个用于动态神经网络和深度学习的开源软件库，基于`Python`和`Torch`，提供了强大的`GPU`加速和自动微分功能，适合科研和快速开发。
3. `Keras`：一个基于`TensorFlow`的高级神经网络`API`，用纯`Python`编写，提供了简洁和友好的接口，可以轻松构建、训练和评估深度学习模型。

### 1.2 区别

| 框架         | 计算图                                                      | 易用性                                                       | 社区规模                                             |
| :----------- | :---------------------------------------------------------- | :----------------------------------------------------------- | :--------------------------------------------------- |
| `Tensorflow` | 静态计算图                                                  | 复杂和繁琐，需要特殊的工具和占位符                           | 最大最活跃，用户基数最多                             |
| `Pytorch`    | 动态计算图，可以根据需要修改和执行节点                      | 简洁和自然，与`Python`语言融合紧密，易于调试                 | 相对较新，但有很多忠实的用户和开发者，社区规模在增长 |
| `Keras`      | 高级`API`，可以在其他框架的后端上运行，使用静态或动态计算图 | 简单和友好，提供了一系列定义明确的参数和层，轻松构建、训练和评估模型 | 依赖于其他框架的后端，社区规模相对较小               |

### 1.3 学习路线

`Keras` -> `Pytorch` / `Tensorflow`

## 二、学习资料

[`Keras` 中文文档](https://keras.io/zh/)

[`Keras` 入门](https://www.zhihu.com/question/51767944)

`Keras` 速查表
……

## 三、基于Deep Speaker的声纹识别模块

### 3.1 概述

Deep Speaker是一种神经说话人嵌入系统，将话语映射到一个超球面上，说话人相似度由余弦相似度衡量。由Deep Speaker生成的嵌入可以用于许多任务，包括说话人识别、验证和聚类。

### 3.2 材料

 [论文](https://arxiv.org/pdf/1705.02304.pdf) | [非官方实现](https://github.com/philipperemy/deep-speaker) | [预训练模型](https://drive.google.com/open?id=18h2bmsAWrqoUMsh_FQHDDxp7ioGpcNBa)

### 3.3 添加 `TDNN` 

`TDNN`（Time Delay Neural Network）可以通过对时间序列数据进行建模和特征提取，帮助模型更好地捕捉语音信号中的时序信息和频率信息，提高模型对语音信号的建模能力，从而提高声纹识别的准确度。

```python
# Define the TDNN model.
tdnn_input_shape = (512, 1)  # The input shape for the TDNN model should be based on the output of the base model.
tdnn_output_dim = 128  # You can adjust this value based on your needs.
tdnn_model = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=tdnn_input_shape),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=128, kernel_size=3, activation='relu'),
    MaxPooling1D(pool_size=2),
    Conv1D(filters=256, kernel_size=3, activation='relu'),
    GlobalAveragePooling1D(),
    Dense(units=tdnn_output_dim)
])
tdnn_model.compile(loss='mse', optimizer='adam')
```

### 3.4 问题

程序失去了识别能力

### 3.5 原因

1. **（*）`TDNN` 模型的结构不合适**

   `TDNN` 模型需要根据不同的应用场景和数据集进行调整。如果模型的层数、卷积核大小、全连接层结构等参数不合适，可能会导致模型在声纹识别任务上表现不佳。

2. **没有正确地融合 `TDNN` 模型和基本模型的输出**

   在使用 `TDNN` 模型之前，需要将基本模型的输出传递给 `TDNN` 模型进行进一步处理。如果没有正确地融合这些输出，可能会导致模型的性能下降。

### 3.6 修改

```python
# Define the TDNN model.
tdnn_input_shape = (512,)  # The input shape for the TDNN model should be based on the output of the base model.
tdnn_output_dim = 128  # You can adjust this value based on your needs.
tdnn_model = Sequential([
    Dense(units=512, activation='relu', input_shape=tdnn_input_shape),
    BatchNormalization(),
    Dropout(0.5),
    Dense(units=512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(units=tdnn_output_dim)
])
tdnn_model.compile(loss='mse', optimizer='adam')
```

音频属于**时序数据**，所以在开始时， `TDNN` 模型使用 `Conv1D` 层来提取特征，通过多个卷积和池化层，最后使用全局平均池化层来对特征进行整合，然后再进行最终的预测。

> **Mel频率倒谱系数**（Mel Frequency Cepstral Coefficients，简称MFCC）是一种音频特征提取方法，常用于语音识别、声纹识别等任务中。在**MFCC算法**中，通常会使用Mel滤波器组对音频信号进行滤波，得到一系列Mel频率谱，然后再对这些谱进行离散余弦变换（Discrete Cosine Transform，DCT），得到一组MFCC系数。
>
> Mel滤波器组是一组在Mel尺度上等距分布的滤波器，它们的中心频率与人耳感知到的声音高低程度更接近。Mel尺度是一种基于人耳听觉特性而设计的频率尺度，它的刻度更符合人类听觉感知的特点，比如相邻两个Mel频率之间的差异对于人类听觉来说是相等的。
>
> Mel滤波器组通常用于将音频信号从时域转换到频域，得到一组Mel频率谱。Mel频率谱是一种对于音频信号频率特征的表示方法，它与人耳感知的声音高低程度更接近，比原始频谱更加适合于人类听觉感知。在MFCC算法中，通常会使用一组Mel滤波器对音频信号进行滤波，得到一组Mel频率谱，然后再对这些谱进行DCT变换，得到一组MFCC系数，用于表示音频信号的频谱特征。
>
> Mel频率谱通常使用图像的形式进行展示，称为Mel频谱图或Mel图。Mel图是一种将音频信号的频谱特征可视化的方式，它可以直观地展示音频信号的频率分布情况，便于进行分析和处理。

图像属于非时序数据，使用更适合非时序数据全连接层来构建 `TDNN` 模型。

修改后的 `TDNN` 模型使用了**全连接层**来进行特征提取和预测。它包括**两个隐藏层和一个输出层**，其中包括批量标准化层和 Dropout 层，以减少过拟合。

### 3.7 导入模型

```python
# Define the base model.
base_model = DeepSpeakerModel()
base_model.m.load_weights("ResCNN_triplet_training_checkpoint_265.h5", by_name=True)

# Define the TDNN model and load weights from HDF5 file.
tdnn_input_shape = (512,)  # The input shape for the TDNN model should be based on the output of the base model.
tdnn_output_dim = 128  # You can adjust this value based on your needs.

# Load the saved pre-trained model and extract the model itself.
# tdnn_model = torch.load('pretrained_average_9_25.pt', map_location=torch.device('cpu'))
tdnn_model = tf.keras.models.load_model('classifier.ckpt')
```

> 失败

`Model Zoo` 和 `Hugging face` 上下载的预训练模型在输入输出的参数上都有很多不匹配的地方，没有办法直接在代码上通过格式转换规避错误，无法使用，需要自己进行模型训练。

### 3.8 后续

1. 训练 `TDNN` 模型
2. 对 `deep speaker` 预训练模型进行 `finetune` 
3. 模型格式转换
4. 模型输入输出数据格式转换
5. `TDNN` 与 `deep speaker` 模型融合

## 四、基于GAN的对抗样本生成模块

### 4.1 演示

[demo](http://127.0.0.1:5500/index.html)

### 4.2 运行代码文件

> 根据原始文件

```python
from util.parser import get_parser
from util.config import Config
from util.mytorch import same_seeds
from agent.inferencer import Inferencer


def get_args():
    parser = get_parser(description='Inference')

    # required
    parser.add_argument('--load', '-l', type=str, help='Load a checkpoint.', required=True)
    parser.add_argument('--source', '-s', help='Source path. A .wav file or a directory containing .wav files.',
                        required=True)
    parser.add_argument('--target', '-t', help='Target path. A .wav file or a directory containing .wav files.',
                        required=True)
    parser.add_argument('--output', '-o', help='Output directory.', required=True)

    # config
    parser.add_argument('--config', '-c', help='The train config with respect to the model resumed.',
                        default='./config/train_again-c4s.yaml')
    parser.add_argument('--dsp-config', '-d', help='The dsp config with respect to the training data.',
                        default='./config/preprocess.yaml')

    # dryrun
    parser.add_argument('--dry', action='store_true', help='whether to dry run')
    # debugging mode
    parser.add_argument('--debug', action='store_true', help='debugging mode')

    # seed
    parser.add_argument('--seed', type=int, help='random seed', default=961998)

    # [--log-steps <LOG_STEPS>]
    parser.add_argument('--njobs', '-p', type=int, help='', default=4)
    parser.add_argument('--seglen', help='Segment length.', type=int, default=None)

    return parser.parse_args()


if __name__ == '__main__':
    # config
    args = get_args()
    config = Config(args.config)
    same_seeds(args.seed)
    args.dsp_config = Config(args.dsp_config)

    # build inferencer
    inferencer = Inferencer(config=config, args=args)

    # inference
    inferencer.inference(source_path=args.source, target_path=args.target, out_path=args.output, seglen=args.seglen)
```

### 4.3 运行

```
python inference.py --load "./checkpoints/again/c4s/again-c4s_100000.pth" --source "./data/wav48/p225/p225_001.wav" --target "./data/wav48/p226/p226_001.wav" --output "./output"
```

### 4.4 材料

[论文](https://arxiv.org/abs/2011.00316)

### 4.5 后续

自动运行程序，使用[`LibriSpeech`](http://www.openslr.org/12/)，生成对抗样本，用于后续判别器训练

## 五、模块连接测试

### 5.1 配置运行环境

#### 5.1.1 配置过程记录

1. 打开 `PyCharm`，选择其中一个项目，打开该项目对应的虚拟环境。

2. 在 `PyCharm` 中打开终端，执行以下命令安装 `pipreqs` 工具：

   ```bash
   pip install pipreqs
   ```

3. 生成一个新的 `requirements.txt` 文件，该文件包含当前虚拟环境中所有已安装的 Python 模块：

   ```
   pipreqs /path/to/project --encoding=utf-8
   pipreqs D:/AAA/lab/tdnn --encoding=utf-8
   ```

   其中 `/path/to/project` 是你要合并的项目的路径。执行该命令后，会在项目的根目录下生成一个名为 `requirements.txt` 的文件，该文件包含当前虚拟环境中所有已安装的 Python 模块及其版本信息。

4. 切换到另一个项目，打开该项目对应的虚拟环境。

5. 在 `PyCharm` 中打开终端，执行以下命令安装 `pip-tools` 工具：

   ```bash
   pip install pip-tools
   ```

6. 将第一步中生成的 `requirements.txt` 文件复制到第二个项目的根目录下。

7. 在第二个项目的根目录下创建一个新的文件 `requirements.in`，将第一步中生成的 `requirements.txt` 文件的内容复制到该文件中。

8. 在终端中执行以下命令，生成一个新的 `requirements.txt` 文件，该文件包含了两个项目中所有的依赖项：

   ```
   pip-compile requirements.in
   ```

   创建一个新项目，安装 `requirements.txt`

   ```
   pip install -r requirements.txt
   ```

#### 5.1.2 配置问题

在最后一步遇到问题：

运行时警告：

```
(venv) PS D:\AAA\lab\tdnn> pip-compile requirements.in
WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.
```

> 由于 `pip-tools` 1.16.0 版本开始默认使用新的依赖解析器 `pip-tools resolver`，而不是旧的依赖解析器 `pip-compile resolver`。新的依赖解析器使用了更为准确的依赖关系解析算法，在某些情况下可能会比旧的解析器更慢，但是可以避免一些依赖冲突和版本不兼容的问题。

```
Skipped pre-versions: 1.23.0rc1, 1.23.0rc1, 1.23.0rc2, 1.23.0rc2, 1.23.0rc3, 1.23.0rc3, 1.24.0rc1, 1.24.0rc1, 1.24.0rc2, 1.24.0rc2
There are incompatible versions in the resolved dependencies:
numpy<=1.24.4,>=1.23.5 (from -r requirements.in (line 1))
numpy>=1.17.3 (from pandas==1.3.0->-r requirements.in (line 7))
numpy~=1.19.2 (from tensorflow==2.6.0->-r requirements.in (line 11))
numpy>=1.15.0 (from librosa==0.8.1->-r requirements.in (line 5))
```

但是再等待一段时间后还是因为 `numpy` 版本不兼容，`requirements.txt` 生成失败。

#### 5.1.3 后续

尝试更换 `numpy` 版本，解决 `pip-tools` 依赖解析器的依赖冲突问题。

最后五个模块均开发完成后，如果依赖冲突无法解决，则手动配置虚拟环境依赖。

### 5.2 运行代码

```python
import subprocess

# 运行 inference.py
inference_path = "./advGAN/inference.py"
load_path = "./checkpoints/again/c4s/again-c4s_100000.pth"
source_path = "./data/wav48/p225/p225_001.wav"
target_path = "./data/wav48/p226/p226_001.wav"
output_path = "./output"
subprocess.run(['python', inference_path, '--load', load_path, '--source', source_path, '--target', target_path, '--output', output_path])

# 运行 tdnn.py
tdnn_path = "./tdnn/tdnn.py"
subprocess.run(['python', tdnn_path])
```

由于依赖存在冲突，大虚拟环境还未配置，暂时无法运行。